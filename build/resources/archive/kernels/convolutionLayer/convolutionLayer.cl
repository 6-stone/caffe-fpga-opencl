/**
    @author Xinyu Zhang
*/

/**
 * dType is the data type used for weight, bias, neuron value, for example:
 *
 * floating point:
 * - half (16bit)
 * - float (32bit)
 * - double (64bit)
 *
 * fixed point:
 * - char (8bit)
 * - short (16bit)
 * - int (32bit)
 * - long (64bit)
 */
typedef float dType;


/**
 * Ping Pong Buffer is used to parallelize Computation/External Memory Loading.
 * But it needs OpenCL 2.0 support of global device buffer. (Prior version only supports global device constant buffer.)
 * Xilinx FPGA supports it, AMD CPU/GPU supports it, but Intel/Nvidia not.
 *
 */
// #define USE_PING_PONG_BUFFER


/**
 * Maximum number of weight/bias/intermediate_feature_map data to load from externel memory,
 * available from [nework].json file generated by python convertor.
 *
 * The size of on-chip memory buffer depends upon this.
 *
 * Maximum Weight(Filter/Kernel) size.
 */
#define MAX_WEIGHT_DATA_NUM  405000
/**
 * Maximum Bias size.
 */
#define MAX_BIAS_DATA_NUM  100
/**
 * Maximum Intermediate Feature Map size.
 */
#define MAX_OUTPUT_FM_DATA_NUM 11520

/**
 * If set, then localsize is it, which will optimize the kernel.
 * This setting fixes the global pooling size, but it's not that bad.
 */
#define SUGGEST_POOLING_SIZE 2

/**
 * Assertion will work in DEBUG mode.
 */
#define DEBUG

////////// ABOVE ARE CONFIGURATIONS, DO NOT CHANGE BELOW IF YOU DON'T KNOW WHAT YOU ARE DOING //////////

#ifdef USE_PING_PONG_BUFFER
    /**
     * If use ping pong buffer, the buffer size should be 2.
     */
    #define pingPongSize 2

    /**
     * Weight Cache
     */
    global dType weightCache[pingPongSize][MAX_WEIGHT_DATA_NUM];
    /**
     * Bias Cache
     */
    global dType biasCache[pingPongSize][MAX_BIAS_DATA_NUM];
    /**
     * Intermediate Feature Map Cache
     */
    global dType fmCache[pingPongSize][MAX_OUTPUT_FM_DATA_NUM];

    /**
     * [computingPhase is used for ping pong buffer, {true, false, true, false, true ...}]
     * @type {[bool]}
     *
     * MEM_PHASE is the buffer id for data write for every phase.
     * COMPUTE_PHASE is the buffer id for data read for every phase.
     */
    #define MEM_PHASE               (computingPhase == false)    // 1, 0
    #define COMPUTE_PHASE           (computingPhase == true)  // 0, 1
    #define BIAS_MEM                biasCache[MEM_PHASE]
    #define WEIGHT_MEM              weightCache[MEM_PHASE]
    #define FM_MEM                  fmCache[MEM_PHASE]
    #define BIAS_COMPUTING          biasCache[COMPUTE_PHASE]
    #define WEIGHT_COMPUTING        weightCache[COMPUTE_PHASE]
    #define FM_COMPUTING            fmCache[COMPUTE_PHASE]
    #define DATA_LAYER_FM_MEM       outputFeatureMap
#else
    #define FM_MEM                  outputFeatureMap
    #define BIAS_COMPUTING          nextBias
    #define WEIGHT_COMPUTING        nextWeight
    #define FM_COMPUTING            inputFeatureMap
#endif


/**
 * C Macro Stringify, borrow from @link below
 * http://stackoverflow.com/questions/2191316/macro-for-concatenating-two-strings-in-c.
 */
#define STR(x) #x

#define FINAL_RESULT pooling ? workItemResult : maxValue
#define RELU(x) (relu ? ( (x) > 0 ? (x) : 0 ) : (x))

/**
 * C Macro ASSERT, if condition is true, then print
 * "ASSERT FAILURE: ${condition}"
 */
#define ASSERT(condition) \
  if (!(condition)) { printf("ASSERT FAILURE: " STR(condition) ); return; }


/**
convolutionLayer kernel, one kernel to rule them all.
Demonstration of computation phase switching.
    1. dataOnly      [1]  -> Store image in 0.
    2. conv          [0]  -> Read from computing phase(0), store in memory phase(1)
    3. conv          [1]  -> Read from computing phase(1), store in memory phase(0)
    4. data/label    [0]  -> Read from computing phase(0), store in outputFeatureMap, new image store in memory (1).
    5. conv          [1]  -> Read from computing phase(1), store in memory phase(0)
    6. {continue...}
*/
__kernel
#ifdef __xilinx__
#ifdef SUGGEST_POOLING_SIZE
/**
 * Set a fixed pooling size.
 */
__attribute__ ((reqd_work_group_size(SUGGEST_POOLING_SIZE, SUGGEST_POOLING_SIZE,1)))
#endif
#endif
void convolutionLayer(/**
                              * Parameters for the computation:
                              *
                              * ** All the Real Data **
                              *  @param{inputFeatureMap}
                              *  @param{outputFeatureMap}
                              *  @param{nextWeight}
                              *  @param{nextBias}
                              *
                              * ** Computation Paramter **
                              *  @param{convStride}             - Stride for this convolution.
                              *  @param{dilation}               - Dilation support, for segmentation usage.
                              *  @param{kernelSize}             - Size of current Kernel.
                              *  @param{convPad}                - Pad of this convolution, use to fix inputFeatureMap{Height, Width}.
                              *  @param{inputChannel}           - Input Feature Map Channel.
                              *  @param{inputFeatureMapHeight}  - First dimension of the 2D input map:  A[x][], input of the conv, unpadded
                              *  @param{inputFeatureMapWidth}   - Second dimension of the 2D input map: A[][x], input of the conv, unpadded
                              *  @param{outputChannel}          - Output Feature Map Channel {Number of Filters}.
                              *  @param{outputFeatureMapHeight} - First dimension of the 2D output map:  A[x][], output of the conv{not pool, not padd}.
                              *  @param{outputFeatureMapHeight} - Second dimension of the 2D output map:  A[][x], output of the conv{not pool, not padd}.
                              *  @param{relu}                   - Use relu for the final result or not.
                              *  @param{pooling}                - Use Pooling or not.
                              *  @param{poolingSize}            - Pooling for output of the convolution, equal to localsize.
                              *  @param{nextConvPadding}        - Padding for next convolution. {Conv->Pool->Relu}--(pad)->NextConv
                              *
                              * ** External Image Loading Paramter **
                              *  @param{dataLayer}              - Load from inputFeatureMap or not.
                              *  @param{dataOnly}               - Only load from inputFeatureMap or not (load and finish).
                              *  @param{dataChannel}            - Number of Channel of External Image
                              *  @param{dataHeight}             - Height of Image
                              *  @param{dataWidth}              - Width of Image
                              *  @param{dataScale}              - Scale constant of the image
                              *
                              * ** External Weight/Bias Loading Paramter
                              *  @param{nextInputChannel}       - Input Channel of Loading Weight/Bias
                              *  @param{nextKernelSize}         - Loading Kernel Size
                              *  @param{nextOutputChannel}      - Output Channel of Loading Weight/Bias
                              *
                              * ** Phase ID **
                              *  @param{computingPhase}
                              */
                              /** All the Real Data */
                              __global dType *inputFeatureMap,
                              __global float *outputFeatureMap,
                              __global float *nextWeight,
                              __global float *nextBias,

                              /** Computation Paramter */
                              const unsigned int convStride,
                              const unsigned int dilation,
                              const unsigned int kernelSize,
                              const unsigned int convPad,
                              const unsigned int inputChannel,
                              const unsigned int inputFeatureMapHeight,
                              const unsigned int inputFeatureMapWidth,
                              const unsigned int outputChannel,
                              const unsigned int outputFeatureMapHeight,
                              const unsigned int outputFeatureMapWidth,
                              const bool relu,
                              const bool pooling,
                              const unsigned int poolingSize,
                              const unsigned int nextConvPadding,

                              /** External Image Loading Paramter */
                              const bool dataLayer,
                              const bool dataOnly,
                              const unsigned int dataChannel,
                              const unsigned int dataHeight,
                              const unsigned int dataWidth,
                              const dType dataScale,

                              /** External Weight/Bias Loading Paramter */
                              const unsigned int nextInputChannel,
                              const unsigned int nextKernelSize,
                              const unsigned int nextOutputChannel,

                              /** Phase Id */
                              const bool computingPhase)
{
    /**
     * ** Important Dimension Variable **
     */
    // size
    unsigned int globalSize_0 = get_global_size(0);
    unsigned int globalSize_1 = get_global_size(1);
    unsigned int globalSize_2 = get_global_size(2);
    unsigned int localSize_0 = get_local_size(0);
    unsigned int localSize_1 = get_local_size(1);
    unsigned int localSize_2 = get_local_size(2);
    unsigned int groupSize_0 = get_num_groups(0);
    unsigned int groupSize_1 = get_num_groups(1);
    unsigned int groupSize_2 = get_num_groups(2);
    unsigned int globalSize = globalSize_0*globalSize_1*globalSize_2;
    // global id
    unsigned int gid_0 = get_global_id(0);  // Channel Id
    unsigned int gid_1 = get_global_id(1); //  Height  Id
    unsigned int gid_2 = get_global_id(2); //  Width   Id
    unsigned int gid = gid_0 * globalSize_1 * globalSize_2
                     + gid_1 * globalSize
                     + gid_2;
    // local id
    unsigned int lid_0 = get_local_id(0);  //  === 1
    unsigned int lid_1 = get_local_id(1); //   Height Id Inside Pooling Kernel
    unsigned int lid_2 = get_local_id(2); //   Width Id Inside Pooling Kernel
    // group id
    unsigned int groupid_0 = get_group_id(0);  //  Channel Id === gid_0
    unsigned int groupid_1 = get_group_id(1); //   Height Id of the Pooling Kernel
    unsigned int groupid_2 = get_group_id(2); //   Width Id Inside Pooling Kernel
    /** Dimension Variable ENDS */


    /**
     * Dimension Check and Hints.
     */
    // local size should follow our pooling rules.
    ASSERT(localSize_0 == 1)
    if(pooling){
        ASSERT(localSize_1 == poolingSize) //poolingSize = localSize_1
        ASSERT(localSize_2 == poolingSize)
    }
    // Group size should be like the partition id based on pooling
    ASSERT(gid_0 == groupid_0)
    ASSERT(groupid_1 == ((unsigned int)gid_1/poolingSize))
    ASSERT(groupid_2 == ((unsigned int)gid_2/poolingSize))
    ASSERT(groupSize_1 == ((unsigned int)(outputFeatureMapHeight/poolingSize)))
    ASSERT(groupSize_2 == ((unsigned int)(outputFeatureMapWidth/poolingSize)))
    // Global id for each output pixel from this layer
    ASSERT(globalSize_0 == outputChannel)
    ASSERT(globalSize_1 == outputFeatureMapHeight)
    ASSERT(globalSize_2 == outputFeatureMapWidth)
    /**  Dimension Check and Hints ENDS */


    #ifdef USE_PING_PONG_BUFFER
        /**
         * Load weight and Bias for next computation
         */
        if(gid < nextOutputChannel) {
            // The first few work items load bias data.
            BIAS_MEM[gid] = nextBias[gid];
        }
        // total number of weight data
        __private unsigned int totalWeightToLoad =  nextKernelSize * nextKernelSize * nextInputChannel * nextOutputChannel;
        // responsibility of this work item
        __private unsigned int weightLoadCounter = gid;
        while(weightLoadCounter < totalWeightToLoad){
            // value storage
            WEIGHT_MEM[weightLoadCounter] = nextWeight[weightLoadCounter];
            // load chunk by chunk.
            weightLoadCounter += globalSize;
        }
        // No need to put barrier here.
        /** Load weight and Bias for next computation ENDS */

        /** load new image if needed. */
        if(dataLayer){
            __private unsigned int dataLoadCounter = gid;
            __private unsigned int totalDataToLoad = dataChannel*dataHeight*dataWidth;
            while(dataLoadCounter < totalDataToLoad){
                FM_MEM[dataLoadCounter] = inputFeatureMap[dataLoadCounter] * dataScale;
                dataLoadCounter += globalSize;
            }
            if(dataOnly)
                return;
        }
        /** load new image if needed ENDS */
    #endif

    /** Convolution computation. */
    __private unsigned int dilatedKernelSize = (kernelSize-1)*dilation+1;
    __private dType workItemResult = 0;
    __private unsigned int paddedInputFeatureMapWidth = inputFeatureMapWidth + (2*convPad);
    __private unsigned int paddedInputFeatureMapHeight = inputFeatureMapHeight + (2*convPad);
    for (unsigned int c = 0; c<inputChannel; c++){
        for(unsigned int i = 0; i<dilatedKernelSize; i++)
            for(unsigned int j = 0; j<dilatedKernelSize; j++)
                workItemResult += (FM_COMPUTING[c*(paddedInputFeatureMapWidth*paddedInputFeatureMapHeight)         //accessing which channel
                                                            +(gid_1*convStride + i*dilation) * paddedInputFeatureMapWidth
                                                            +(gid_2*convStride + j*dilation)]                                  //
                                                            *
                                   WEIGHT_COMPUTING[gid_0 * kernelSize * kernelSize * inputChannel // which filter
                                                            + c * (kernelSize*kernelSize)   //which channel
                                                            + i * kernelSize
                                                            + j]);  // inside this channel, 2d filter.
    }
    workItemResult += BIAS_COMPUTING[gid_0];
    barrier(CLK_LOCAL_MEM_FENCE);
    /** Convolution computation ENDS */

    /** Post Convolution: find the max among work group, max pooling, and then relu */
    __local dType maxValue;
    maxValue = FLT_MIN;
    if(pooling){
        for (unsigned int i = 0; i<localSize_1;i++)
            for(unsigned int j = 0; j<localSize_2;j++){
                barrier(CLK_LOCAL_MEM_FENCE);
                if(lid_1 == i && lid_2 == j){
                    maxValue = maxValue > workItemResult? maxValue : workItemResult;
            }
        }
        barrier(CLK_LOCAL_MEM_FENCE);
        maxValue =  RELU(maxValue);
    } else{
        workItemResult = RELU(workItemResult);
    }

    /** Post Convolution ENDS */


    /** Result Exportation */
    __private unsigned int outputIndex;
    __private unsigned int outputFeatureMapHeightPooled = outputFeatureMapHeight/poolingSize;
    __private unsigned int outputFeatureMapWidthPooled =  outputFeatureMapWidth/poolingSize;
    __private unsigned int outputid_0, outputid_1, outputid_2;
    if(pooling){
        outputid_0 = groupid_0;
        outputid_1 = groupid_1;
        outputid_2 = groupid_2;
    }else{
        outputid_0 = gid_0;
        outputid_1 = gid_1;
        outputid_2 = gid_2;
    }
    #ifdef USE_PING_PONG_BUFFER
        // Parrellel Padding
        if (outputid_1 < nextConvPadding || outputid_2 < nextConvPadding){
                FM_MEM[outputid_0 * outputFeatureMapHeightPooled * outputFeatureMapWidthPooled
                        + outputid_1 * outputFeatureMapWidthPooled
                        + outputid_2] = 0;
            }
            if ((outputFeatureMapWidthPooled - outputid_1) < nextConvPadding || (outputFeatureMapHeightPooled - outputid_2) < nextConvPadding){
                FM_MEM[outputid_0 * outputFeatureMapHeightPooled * outputFeatureMapWidthPooled
                        + (outputid_1 + 2*nextConvPadding) * outputFeatureMapWidthPooled
                        + (outputid_2 + 2*nextConvPadding)] = 0;
            }


        outputIndex = outputid_0*(outputFeatureMapHeightPooled * outputFeatureMapWidthPooled)
                               + (outputid_1 + nextConvPadding) *outputFeatureMapWidthPooled
                               + (outputid_2 + nextConvPadding);
        // Data Storage
        if(dataLayer){
            DATA_LAYER_FM_MEM[outputIndex] = FINAL_RESULT;
        }else{
            FM_MEM[outputIndex] = FINAL_RESULT;
        }
    #else
        // If not use PingPong Buffer, skip padding and you should pad them in CPU!!
        outputIndex = outputid_0 * (outputFeatureMapHeightPooled * outputFeatureMapWidthPooled)
                                + outputid_1 * outputFeatureMapWidthPooled
                                + outputid_2;
        FM_MEM[outputIndex] = FINAL_RESULT;
    #endif
    /** Result Exportation ENDS */
}



/**
 * OPTIMIZATION TIPS:
 * - xcl_pipeline_workitems
 * - xcl_pipeline_loop
 * - opencl_unroll_hint(factor)
 */
